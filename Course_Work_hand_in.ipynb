{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "6.1 Define the task\n",
    "the task is to traing a ai modle to predict what difrent fasion items are \n",
    "\n",
    "is will predict what 10 types of fasion items as are by trainging to a nural network on prexisting data and then give it new data \n",
    "\n",
    "\n",
    "\n",
    "<h1>66.1.1 Frame the problem</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- hat are you trying to predict?  -->\n",
    "i will start of by doing some reserch on how to predict data using a prexisting data set call ed mnist \n",
    "\n",
    "then in will us the reserch that i learned from predicting numbers and apply the same tecneq to predict fasion  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- And is the data (samples and labels) available? -->\n",
    "\n",
    "<!-- What is the type of the problem?  -->\n",
    "\n",
    "<!-- Binary classification, single label multiclass, multilabel multiclass, regression?  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Hypotheses -->\n",
    "\n",
    "<!-- - the outputs can be predicted from the inputs; -->\n",
    "<!-- - the available data is sufficiently informative. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypotheses might not be met even if your problem is well defined.\n",
    "\n",
    "For example, an attempt to the predict stock prices from the recent past will likely fail because recent prices contain very little predictive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonstationariness\n",
    "\n",
    "Nonstationary problems have a time ordering.\n",
    "\n",
    "For example, a clothing recommender will have to take the time of year into account (swimsuits in summer, coats in winter).\n",
    "\n",
    "Either gather information from a period when the problem is stationary (a succession of summers) or train the model on recent trends (the past few weeks) or on all data but include the time of year as an input.\n",
    "\n",
    "(In the industry this means a lot of retraining and finetuning all the time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third hypothesis: **the future is like the past**.\n",
    "\n",
    "The kind of machine learning we study now **only spots patterns in collected data** – data that lies in the past.\n",
    "\n",
    "We assume, when we use a trained model, that the past is relevant today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.1.2 Collect a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investing in data annotation infrastructure\n",
    "\n",
    "This is more for companies: should you pay people to annotate your data?\n",
    "\n",
    "\n",
    "Fun (?) fact: nobody really knows how many people do this, but comapnies like OpenAI use *armies* of very low-paid workers to annotate and check data. Fairly ironic to think that so many humans are working *for* the AI systems, as it were, even as those systems are supposed to be working *for us*..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beware of non-representative data\n",
    "\n",
    "You should strive to make sure that your **training data** and the **real world (unseen)** data you're using your model on come from the **same distribution**.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "Very clean and well-lit pictures in training, real-world pictures from social media in production, you're in for trouble!\n",
    "\n",
    "Personal issue: trained on literary corpora for a bot interacting with actors. Humans would write fast, make mistakes, etc., in a way that wasn't at all \"literary\"!\n",
    "\n",
    "*Concept drift*: much of the data in the world changes constantly. Chollet's example: recommender system, you can't have the same system now as in a few years ago!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The problem of sampling bias\n",
    "\n",
    "If you based your estimation of who won the election that night just on this one newspaper, you would be wrong!\n",
    "<!-- <img style=\"float:right;height:550px\" src=\"images/chollet.sampling-bias.png\"> -->\n",
    "<img  style=\"float:right;height:550px\" src=\"https://drive.google.com/uc?id=1CnnE3seK1nxxqa17-eoF1-PwIwHad7hg\">\n",
    "\n",
    "<small style=\"position:absolute;bottom:0;right:0\">DLWP, p.184</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.3 Understand your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.4 Choose a measure of success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. **Define** the problem and **collect data** (samples and labels)\n",
    "2. Choose the **success criterion** (**loss**) and monitoring **metrics**\n",
    "3. Chose a **validation protocol** (hold-out...)\n",
    "4. **Prepare your data**\n",
    "5. Check your **baselines**\n",
    "5. **Train**  \n",
    "   a. a first underfitting model (still with statistical power, not a total wreck)  \n",
    "   b. a larger overfitting model  \n",
    "7. **Tune**  \n",
    "   a. **Regularise** the overfitting model  \n",
    "   b. **Tweak** hyperparameters based on performance on the **validation set**  \n",
    "8. **Retrain on the entire training set** and evaluate on your **unseen test set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "- Different architectures (number of layers, units per layer)  \n",
    "- Dropout\n",
    "- L1/L2 regularisation\n",
    "- Other hyperparameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.3 Understand your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.4 Choose a measure of success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important? Accuracy? Precision? Something else?\n",
    "\n",
    "The measure of success, whatever it is, will influence the choice of loss function.\n",
    "\n",
    "You may have to design your own loss function – just remember that backpropagation demends smoothness.\n",
    "\n",
    "Once your aims are clear, you have to decide how to measure progress as you tune your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.2 Develop a model\n",
    "\n",
    "## 6.2.1 Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation \n",
    "\n",
    "- all inputs must be floating point tensors;\n",
    "- for example, words in text data  are one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Normalisation\n",
    "\n",
    "- digit classification – integer vector elements in $[0, 255]$ normalised by casting as floats and dividing by 255 to give floating point data in $[0, 1]$;\n",
    "- house prices – the data was normalised so that each feature had mean of 0 and std of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent works best if data is homogeneous and constrained.\n",
    "\n",
    "Constrain\n",
    "\n",
    "- aim to put data values in $[0, 1]$ or $[-1, 1]$;\n",
    "- weights and biases are initialised with small values otherwise a few large inputs might dominate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity \n",
    "\n",
    "- all features take values in roughly the same range;\n",
    "- inhomogeneity can trigger large gradient updates and hamper convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation to a mean of 0 and standard deviation of 1 can help but is not always necessary;\n",
    "\n",
    "(E.g. we didn't normalise the digit data in this way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "- sometimes a data point lacks a feature;\n",
    "- you could fill the missing data with zeros (as long as 0 isn't already meaningful);\n",
    "- the network will learn to ignore 0's because the 0's don't correlate with anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perhaps the test data has missing values but the training data does not.\n",
    "\n",
    "In that case, duplicate some training data and randomly place 0's in some features (but leave the test data alone!) so that the network can learn to ignore missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "## 6.2.2 Choose an evaluation protocol\n",
    "\n",
    "Our goal is to *generalise* well. For that, we need to measure how well we are doing outside training.\n",
    "\n",
    "- Maintaining a **holdout validation set**: this is the way to go when you have plenty of data;\n",
    "- Doing **K-fold cross-validation**: this is the right choice when you have too few samples for holdout validation to be reliable;\n",
    "- Doing **iterated K-fold validation**: the most accurate, but also the most expensive, method when little data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.2.3 Beat a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim for *statistical power* – a small model that can beat a common sense check\n",
    "\n",
    "- for example, in digit classification, a baseline algorithm merely assigns labels at random – our small model must beat 10%\n",
    "- For the movie reviews, our model must perform better than 50% accuracy\n",
    "\n",
    "(Assuming that MNIST and IMDB have an equal distribution of class membership) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ut sometimes, after much experimentation, we cannot beat the baseline.\n",
    "\n",
    "Perhaps one or more of the hypotheses are false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is no prescription for model design, but there are guidelines.\n",
    "\n",
    "- the smallest capacity model with statistical power;\n",
    "- `rmsprop` optimiser with default learning rate;\n",
    "- loss and last layer activation according to the table: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 12:08:49.106318: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-30 12:08:49.106348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (splinter0666): /proc/driver/nvidia/version does not exist\n",
      "2022-11-30 12:08:49.106707: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  learning rate 0.01 momentum 0.1 1 . 1\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2518 - accuracy: 0.9266\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1028 - accuracy: 0.9698\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0675 - accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0486 - accuracy: 0.9849\n",
      "Epoch 5/5\n",
      " 57/469 [==>...........................] - ETA: 0s - loss: 0.0372 - accuracy: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 69\u001b[0m\n\u001b[1;32m     59\u001b[0m network\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     60\u001b[0m   optimizer\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mRMSprop(lr\u001b[39m=\u001b[39m(i\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m), momentum\u001b[39m=\u001b[39m(j\u001b[39m/\u001b[39m \u001b[39m10\u001b[39m)),\n\u001b[1;32m     61\u001b[0m   loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     62\u001b[0m   metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[39m# TestArray[0] = TestDat(0,1)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# print(network.compile.metrics, \"teeeeeeeeeeeeeeeeeeeeeee\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# model.compile(optimizer=, loss=, metrics=['accuracy',...])\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# network.compile.evaluate()\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# print(network.compile())\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m history \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n\u001b[1;32m     70\u001b[0m \u001b[39m# print(network.compile)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(history.history['accuracy'][0], \"weeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\")\u001b[39;00m\n\u001b[1;32m     72\u001b[0m TestArray[iterCount] \u001b[39m=\u001b[39m TestDat((i\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m),(j\u001b[39m/\u001b[39m \u001b[39m10\u001b[39m),history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m3\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "\n",
    "# load\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import optimizers\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# preprocess\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "# build\n",
    "from tensorflow.keras import models, layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28, )))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 0-1\n",
    "# lr 0.001 - 0.00\n",
    "\n",
    "class TestDat():\n",
    "    def __init__(self, learning_rate, momentum,accuracy):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "\n",
    "TestArray = [None for c in range(9**2)]\n",
    "\n",
    "# TestArray[0] = TestDat(0,1)\n",
    "# TestArray[1] = TestDat(0,1)\n",
    "\n",
    "\n",
    "# print (TestArray[0].Dat1) # this is Test1\n",
    "# print (TestArray[1].Dat1) # this is Test2    \n",
    "\n",
    "iterCount = 0\n",
    "\n",
    "for i in range (1,9): # (initial, final but not included)\n",
    "  for j in range (1,9): \n",
    "    print(\"test: \",\"learning rate\" , (i/ 100) , \"momentum\", (j/ 10) , i , \".\" , j)\n",
    "\n",
    "    # redefine the var network \n",
    "    network.compile(\n",
    "      optimizer=optimizers.RMSprop(lr=(i/ 100), momentum=(j/ 10)),\n",
    "      loss='categorical_crossentropy', \n",
    "      metrics=['accuracy']\n",
    "    )\n",
    "    # TestArray[0] = TestDat(0,1)\n",
    "    # print(network.compile.metrics, \"teeeeeeeeeeeeeeeeeeeeeee\")\n",
    "    # model.compile(optimizer=, loss=, metrics=['accuracy',...])\n",
    "    # network.compile.evaluate()\n",
    "    # print(network.compile())\n",
    "    history = network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "    # print(network.compile)\n",
    "    # print(history.history['accuracy'][0], \"weeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\")\n",
    "    TestArray[iterCount] = TestDat((i/ 100),(j/ 10),history.history['accuracy'][3])\n",
    "    # if(iterCount == 9^2):\n",
    "    for spit in range (0,iterCount):\n",
    "      print(\"learning_rate: \" ,TestArray[spit].learning_rate, \"momentum: \" ,TestArray[spit].momentum, \"accuracy: \" ,TestArray[spit].accuracy)\n",
    "    iterCount += 1\n",
    "    # history.history['accuracy'][0], \"weeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\"\n",
    "\n",
    "# train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_file = \"/usr/share/dict/words\"\n",
    "WORDS = open(word_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The problem of sampling bias\n",
    "\n",
    "If you based your estimation of who won the election that night just on this one newspaper, you would be wrong!\n",
    "<!-- <img style=\"float:right;height:550px\" src=\"images/chollet.sampling-bias.png\"> -->\n",
    "<img  style=\"float:right;height:550px\" src=\"https://drive.google.com/uc?id=1CnnE3seK1nxxqa17-eoF1-PwIwHad7hg\">\n",
    "\n",
    "<small style=\"position:absolute;bottom:0;right:0\">DLWP, p.184</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use comand in the terminal --> pip install random-word\n",
    "\n",
    "please import this package so that you can generate the words and make the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sustenanceless\n"
     ]
    }
   ],
   "source": [
    "# First pip install random-word\n",
    "from random_word import RandomWords\n",
    "r = RandomWords()\n",
    "\n",
    "# Return a single random word\n",
    "rand_word_incoding = r.get_random_word()\n",
    "\n",
    "print(rand_word_incoding)\n",
    "# Return list of Random words\n",
    "# r.get_random_words()\n",
    "# # Return Word of the day\n",
    "# r.word_of_the_day()\n",
    "\n",
    "# print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trouser', 'unbowered', 'springled', 'trouser', 'traversals', 'cleidorrhexis', 'trouser', 'counterraid', 'bullous', 'trouser', 'minimax', 'aloysia', 'hypernormal', 'trouser', 'trouser', 'trouser', 'chronologies', 'trouser', 'heptatomic', 'enhydrinae', 8, 80]\n",
      "['trouser', 'unbowered', 'springled', 'trouser', 'traversals', 'cleidorrhexis', 'trouser', 'counterraid', 'bullous', 'trouser', 'minimax', 'aloysia', 'hypernormal', 'trouser', 'trouser', 'trouser', 'chronologies', 'trouser', 'heptatomic', 'enhydrinae']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# randint generates a random integar between the first parameter and the second\n",
    "\n",
    "# random.randint(1, 100)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "categorys = ['top', 'trouser', 'pullover', 'dress',\n",
    "             'coat', 'sandle', 'shirt', 'sneaker', 'bag', 'bnkle boot']\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "def genItemDescription(selected_catecogory_num, descreption_of_listing, probablity_of_outcome):\n",
    "    # selected_catecogory_num =  0\n",
    "    # descreption_of_listing = []\n",
    "    actual_outcome_counter = 0\n",
    "\n",
    "    for i in range(20): # (initial, final but not included)\n",
    "        probabelety = random.randint(1, 100)    \n",
    "        if probabelety > probablity_of_outcome:\n",
    "            descreption_of_listing.append(categorys[selected_catecogory_num])\n",
    "            actual_outcome_counter += 1\n",
    "        else:\n",
    "            descreption_of_listing.append(r.get_random_word())\n",
    "\n",
    "    #----------------------------------------------------------------\n",
    "\n",
    "    descreption_of_listing.append(actual_outcome_counter)\n",
    "    descreption_of_listing.append(probablity_of_outcome)\n",
    "    print(descreption_of_listing) \n",
    "\n",
    "    #---------end of function----------------------------------------\n",
    "    return descreption_of_listing\n",
    "\n",
    "#--------------------------------------------------\n",
    "descreption_of_listing = []\n",
    "data_test_vectorizer = genItemDescription(1, descreption_of_listing, 80)\n",
    "\n",
    "    # for i in range(10): # (initial, final but not included)\n",
    "\n",
    "show_slice = data_test_vectorizer[:20]\n",
    "print(show_slice)\n",
    "\n",
    "\n",
    "\n",
    "# rechape_array = np.array(descreption_of_listing)\n",
    "\n",
    "# print(random.randint(1, 100))\n",
    "\n",
    "\n",
    "# label_batch[i].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_ints(s):\n",
    "    s_enc = s.encode()\n",
    "    print(f\"{list(s_enc)}\") # encode into bytes, then turn into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116, 114, 111, 117, 115, 101, 114]\n",
      "['trouser', 'unbowered', 'springled', 'trouser', 'traversals', 'cleidorrhexis', 'trouser', 'counterraid', 'bullous', 'trouser', 'minimax', 'aloysia', 'hypernormal', 'trouser', 'trouser', 'trouser', 'chronologies', 'trouser', 'heptatomic', 'enhydrinae']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    An implementation of the transformations in the above schema.\n",
    "    \"\"\"\n",
    "    def standardize(self, text):\n",
    "        \"\"\"Make lowercase and remove punctuation\"\"\"\n",
    "        text = text.lower()                                # Python built-in list of punctuation characters\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Standardize then split on space\"\"\"\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        \"\"\"Generate token vocabulary from text\"\"\"\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            tokens = self.tokenize(text) # this will standardize\n",
    "            for token in tokens:                                  # first element will be 2\n",
    "                if token not in self.vocabulary:                  # then len(vocab) is 3 →\n",
    "                    self.vocabulary[token] = len(self.vocabulary) # next token is 3, etc.\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to tokens\"\"\"\n",
    "        tokens = self.tokenize(text) # this will standardize\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        \"\"\"Decode tokens to string\"\"\"        \n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "    def one_hot_encode(self, encoded_sequence):\n",
    "        vector = np.zeros((len(encoded_sequence), len(self.vocabulary)))\n",
    "        for i, token in enumerate(encoded_sequence):\n",
    "            vector[i, token] = 1\n",
    "        return vector\n",
    "    \n",
    "    def one_hot_decode(self, one_hot_sequence):\n",
    "        return list(np.where(one_hot_sequence == 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116, 114, 111, 117, 115, 101, 114]\n",
      "['trouser', 'unbowered', 'springled', 'trouser', 'traversals', 'cleidorrhexis', 'trouser', 'counterraid', 'bullous', 'trouser', 'minimax', 'aloysia', 'hypernormal', 'trouser', 'trouser', 'trouser', 'chronologies', 'trouser', 'heptatomic', 'enhydrinae']\n",
      "[2]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# show_slice = data_test_vectorizer[:20]\n",
    "# str_to_ints(show_slice[0])\n",
    "\n",
    "# print(show_slice)\n",
    "\n",
    "show_slice = data_test_vectorizer[:20]\n",
    "str_to_ints(show_slice[0])\n",
    "\n",
    "print(show_slice)\n",
    "\n",
    "# show_slice\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "# dataset = [\n",
    "#     \"I write, erase, rewrite\",\n",
    "#     \"Erase again, and then\",\n",
    "#     \"A poppy blooms.\",\n",
    "# ]\n",
    "vectorizer.make_vocabulary(show_slice)\n",
    "\n",
    "\n",
    "# test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "test_sentence = show_slice[0]\n",
    "\n",
    "\n",
    "\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)\n",
    "\n",
    "\n",
    "one_hot_sentence = vectorizer.one_hot_encode(encoded_sentence)\n",
    "print(one_hot_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "vectorizer1 = Vectorizer()\n",
    "# dataset = [\n",
    "#     \"I write, erase, rewrite\",\n",
    "#     \"Erase again, and then\",\n",
    "#     \"A poppy blooms.\",\n",
    "# ]\n",
    "vectorizer1.make_vocabulary(show_slice)\n",
    "\n",
    "\n",
    "# test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "test_sentence1 = show_slice[0]\n",
    "\n",
    "# test?\n",
    "# test2\n",
    "\n",
    "\n",
    "\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)\n",
    "\n",
    "\n",
    "one_hot_sentence = vectorizer.one_hot_encode(encoded_sentence)\n",
    "print(one_hot_sentence)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# vectorizer = Vectorizer()\n",
    "# dataset = [\n",
    "#     \"I write, erase, rewrite\",\n",
    "#     \"Erase again, and then\",\n",
    "#     \"A poppy blooms.\",\n",
    "# ]\n",
    "\n",
    "# vectorizer.make_vocabulary(show_slice)\n",
    "\n",
    "\n",
    "# test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "# encoded_sentence = vectorizer.encode(test_sentence)\n",
    "# print(encoded_sentence)\n",
    "\n",
    "\n",
    "# one_hot_sentence = vectorizer.one_hot_encode(encoded_sentence)\n",
    "# print(one_hot_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 11:23:34.044289: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 11:23:34.888090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 11:23:34.888135: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 11:23:34.888139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]\n",
      "b'Hello, world!'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jérémie | b'J\\xc3\\xa9r\\xc3\\xa9mie' | [74, 195, 169, 114, 195, 169, 109, 105, 101]\n",
      "jérémie | b'j\\xc3\\xa9r\\xc3\\xa9mie' | [106, 195, 169, 114, 195, 169, 109, 105, 101]\n",
      "jeremie | b'jeremie' | [106, 101, 114, 101, 109, 105, 101]\n",
      "the quick brown fox jumps over the lazy dog\n",
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import unidecode\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def str_to_ints(s):\n",
    "    s_enc = s.encode()\n",
    "    print(f\"{s} | {s_enc} | {list(s_enc)}\") # encode into bytes, then turn into a list\n",
    "\n",
    "s = \"Jérémie\"\n",
    "str_to_ints(s)\n",
    "str_to_ints(s.lower())\n",
    "str_to_ints(unidecode.unidecode(s.lower())) # removing \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s = \"the quick brown fox jumps over the lazy dog\"\n",
    "s_split = s.split(\" \")\n",
    "print(s)\n",
    "print(s_split)\n",
    "\n",
    "# --\n",
    "def n_grams(l, n=2):\n",
    "    n_grams = []\n",
    "    for i in range(0, len(s_split) - n + 1):\n",
    "        n_grams.append(s_split[i:i+n])\n",
    "    return n_grams\n",
    "\n",
    "#--\n",
    "n_grams(s_split, n=2) # each token is two words\n",
    "\n",
    "\n",
    "n_grams(s_split, n=3) # each token is three words\n",
    "\n",
    "\n",
    "vocab = {token:i for i,token in enumerate(sorted(list(set(s_split))))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncoder\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m      5\u001b[0m \u001b[39m# #--------------------------------------------------\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# # jimjam = tf.one_hot(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# #     indices = data_test_vectorizer,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# define example\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None)[source]¶\n",
    "\n",
    "\n",
    "# #--------------------------------------------------\n",
    "# # jimjam = tf.one_hot(\n",
    "# #     indices = data_test_vectorizer,\n",
    "# #     depth = 10,\n",
    "# #     on_value=None,w\n",
    "# #     off_value=None,\n",
    "# #     axis=None,\n",
    "# #     dtype=None,\n",
    "# #     name=None\n",
    "# # )\n",
    "\n",
    "\n",
    "# tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "#     max_tokens=100,\n",
    "#     standardize=\"lower_and_strip_punctuation\",\n",
    "#     split=\"whitespace\",\n",
    "#     ngrams=None,\n",
    "#     output_mode=\"int\",\n",
    "#     output_sequence_length=None,\n",
    "#     pad_to_max_tokens=False,\n",
    "#     vocabulary=None,\n",
    "# )\n",
    "\n",
    "\n",
    "# print(jimjam)\n",
    "\n",
    "\n",
    "\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot',\n",
    "        'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "\n",
    "values = np.array(data)\n",
    "\n",
    "# first apply label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "# now we can apply one hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False,\n",
    "    vocabulary=None,\n",
    "    **kwargs\n",
    ")\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n",
    " max_features = 5000  # Maximum vocab size.\n",
    " max_len = 4  # Sequence length to pad the outputs to.\n",
    " embedding_dims = 2\n",
    "\n",
    "  # Create the layer.\n",
    "#  vectorize_layer = TextVectorizatio...  max_tokens=max_feature...  output_mode='int',\n",
    "# output_sequence_length=max_len)\n",
    "\n",
    "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len,\n",
    "    vocabulary=vocab_data\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "  # dataset to create the vocabulary. You don't have to batch, but for large\n",
    "  # datasets this means we're not keeping spare copies of the dataset.\n",
    " vectorize_layer.adapt(text_dataset.batch(64))\n",
    "\n",
    "  # Create the model that uses the vectorize text layer\n",
    " model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Start by creating an explicit input layer. It needs to have a shape of\n",
    "  # (1,) (because we need to guarantee that there is exactly one string\n",
    "  # input per batch), and the dtype needs to be 'string'.\n",
    " model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "  # The first layer in our model is the vectorization layer. After this\n",
    "  # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "  # indices.\n",
    " model.add(vectorize_layer)\n",
    "\n",
    "  # Now, the model can map strings to integers, and you can add an embedding\n",
    "  # layer to map these integers to learned embeddings.\n",
    " input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n",
    " model.predict(input_data)\n",
    "    #  array([[2, 1, 4, 0   [1, 3, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n",
    ">> > max_len = 4  # Sequence length to pad the outputs to.\n",
    ">> >\n",
    ">> >  # Create the layer, passing the vocab directly. You can also pass the\n",
    ">> >  # vocabulary arg a path to a file containing one vocabulary word per\n",
    ">> >  # line.\n",
    ">> > vectorize_layer = TextVectorization(\n",
    ">> >     max_tokens=max_features,\n",
    "    ...  max_tokens=max_features,\n",
    "    ...  output_mode='int',\n",
    "    ...  output_sequence_length=max_len,\n",
    "    ...  vocabulary=vocab_data)\n",
    ">> >\n",
    ">> >  # Because we've passed the vocabulary directly, we don't need to adapt\n",
    ">> >  # the layer - the vocabulary is already set. The vocabulary contains the\n",
    "# padding token ('') and OOV token ('[UNK]') as well as the passed tokens.\n",
    ">> >\n",
    ">> > vectorize_layer.get_vocabulary()\n",
    "['', '[UNK]', 'earth', 'wind', 'and', 'fire']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import re\n",
    "\n",
    "# # Having looked at our data above, we see that the raw text contains HTML break\n",
    "# # tags of the form '<br />'. These tags will not be removed by the default\n",
    "# # standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# # create a custom standardization function.\n",
    "\n",
    "\n",
    "# def custom_standardization(input_data):\n",
    "#   lowercase = tf.strings.lower(input_data)\n",
    "#   stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "#   return tf.strings.regex_replace(stripped_html,\n",
    "#                                   '[%s]' % re.escape(string.punctuation),\n",
    "#                                   '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextVectorization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TextVectorization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# tf.strings.split().\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m vectorize_layer \u001b[39m=\u001b[39m TextVectorization(\n\u001b[1;32m      5\u001b[0m     output_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m      7\u001b[0m     split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhitespace\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     output_sequence_length\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39m# vectorize_layer.adapt(text_dataset.batch(64))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m jimjam \u001b[39m=\u001b[39m vectorize_layer\u001b[39m.\u001b[39madapt(np\u001b[39m.\u001b[39marray([\u001b[39m'\u001b[39m\u001b[39mfoo bar baz\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfoo bar\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextVectorization' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=500)\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "vectorize_layer.adapt(text_dataset.batch(64))\n",
    "\n",
    "# Next, let's build a model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# tf.keras.layers.TextVectorization(\n",
    "#     max_tokens=None,\n",
    "#     standardize='lower_and_strip_punctuation',\n",
    "#     split='whitespace',\n",
    "#     ngrams=None,\n",
    "#     output_mode='int',\n",
    "#     output_sequence_length=None,\n",
    "#     pad_to_max_tokens=False,\n",
    "#     vocabulary=None,\n",
    "#     idf_weights=None,\n",
    "#     sparse=False,\n",
    "#     ragged=False,\n",
    "#     encoding='utf-8',\n",
    "#     **kwargs\n",
    "# )\n",
    "\n",
    "# #--------------------------------------------------\n",
    "\n",
    "# tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "#     max_tokens=None,\n",
    "#     standardize=\"lower_and_strip_punctuation\",\n",
    "#     split=\"whitespace\",\n",
    "#     ngrams=None,\n",
    "#     output_mode=\"int\",\n",
    "#     output_sequence_length=None,\n",
    "#     pad_to_max_tokens=False,\n",
    "#     vocabulary=None,\n",
    "#     **kwargs\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_object = open(\"filename\", \"mode\")\n",
    "\n",
    "\n",
    "# First pip install random-word\n",
    "\n",
    "f= open(\"guru99.csv\",\"w+\")\n",
    "for i in range(10):\n",
    "    for i in range(20):\n",
    "        f.write(\"This is line %d\\r\\n\" % (i+1))\n",
    "f.close()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m word_site \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.mit.edu/~ecprice/wordlist.10000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m response \u001b[39m=\u001b[39m urllib2\u001b[39m.\u001b[39murlopen(word_site)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "\n",
    "word_site = \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "\n",
    "\n",
    "response = urllib2.urlopen(word_site)\n",
    "txt = response.read()\n",
    "WORDS = txt.splitlines()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
